---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi, this is Tianao Zhang (å¼ å¤©éªœ)'s website!

I am a junior undergraduate student at [Shanghai Jiao Tong University (SJTU)](https://www.sjtu.edu.cn/), majoring in Engineering Mechanics.

I have the privilege of being advised by Prof. [Yulun Zhang](https://yulunzhang.com/) and Prof. [Xiaokang Yang](https://scholar.google.com/citations?user=yDEavdMAAAAJ&hl=en) from [the School of Computer Science](https://www.cs.sjtu.edu.cn/en/).

My research interests focus on **Efficient AI** - Making large models practical and accessible.

I'm open to collaboration and discussion! Feel free to reach out via Email (ztaztazta2785@gmail.com) or WeChat (ZTAZTAZTAZTAZTA)!


# ğŸ”¥ News
- *2025.11*: &nbsp;ğŸ‰ğŸ‰ é¡¹ç›®ã€Šé¢å‘ç«¯ä¾§éƒ¨ç½²çš„å¤§æ¨¡å‹äºŒå€¼åŒ–å‹ç¼©æŠ€æœ¯åŠåº”ç”¨ã€‹è£è·ç¬¬åä¹å±Šâ€œæŒ‘æˆ˜æ¯â€å…¨å›½å¤§å­¦ç”Ÿè¯¾å¤–å­¦æœ¯ç§‘æŠ€ä½œå“ç«èµ›**ç‰¹ç­‰å¥–**ã€‚
- *2025.06*: &nbsp;ğŸ‰ğŸ‰ é¡¹ç›®ã€Šæ–°å‹å¤§è¯­è¨€æ¨¡å‹äºŒå€¼åŒ–å‹ç¼©æŠ€æœ¯ã€‹è£è·ç¬¬åä¹å±Šâ€œæŒ‘æˆ˜æ¯â€ä¸Šæµ·å¸‚å¤§å­¦ç”Ÿè¯¾å¤–å­¦æœ¯ç§‘æŠ€ä½œå“ç«èµ›**ç‰¹ç­‰å¥–**ã€‚
- *2025.01*: &nbsp;ğŸ‰ğŸ‰ Our paper ARB-LLM is accepted by **ICLR 2025**!

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/arb-llm.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ARB-LLM: Alternating Refined Binarizations for Large Language Models](https://arxiv.org/abs/2410.03129)

Zhiteng Li<sup>1</sup>, Xianglong Yan<sup>1</sup>, **Tianao Zhang**, Haotong Qin, Dong Xie, Jiang Tian, Zhongchao Shi, Linghe Kong\*, Yulun Zhang\*, and Xiaokang Yang
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/quant-dllm.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)

**Tianao Zhang**<sup>1</sup>, Zhiteng Li<sup>1</sup>, Xianglong Yan, Haotong Qin, Yong Guo, and Yulun Zhang\*
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/pt2llm.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PT<sup>2</sup>-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)

Xianglong Yan<sup>1</sup>, Chengzhu Bao<sup>1</sup>, Zhiteng Li, **Tianao Zhang**, Kaicheng Yang, Haotong Qin, Ruobing Xie, Xingwu Sun, and Yulun Zhang\*
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/recalkv.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration](https://arxiv.org/abs/2505.24357)

Xianglong Yan<sup>1</sup>, Zhiteng Li<sup>1</sup>, **Tianao Zhang**, Haotong Qin, Linghe Kong, Yulun Zhang\*, and Xiaokang Yang
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/pbs2p.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Progressive Binarization with Semi-Structured Pruning for LLMs](https://arxiv.org/abs/2502.01705)

Xianglong Yan<sup>1</sup>, **Tianao Zhang**<sup>1</sup>, Zhiteng Li, Haotong Qin, and Yulun Zhang\*
</div>
</div>

<!-- 
[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**
- -->

# ğŸ– Honors and Awards
- Tongsheng Scholarship (Top 1%) åŒå£°å¥–å­¦é‡‘ (2025)
- Zhiyuan Honors Scholarship (Top 5%) è‡´è¿œè£èª‰å¥–å­¦é‡‘ (2023,2024,2025)
- Excellent Undergraduate Scholarship (Top 10%) æœ¬ç§‘ç”Ÿä¼˜ç§€å¥–å­¦é‡‘ (2024, 2025)


# ğŸ“– Educations
- *2023.09 - now*, B.Eng. in [Shanghai Jiao Tong University](https://www.sjtu.edu.cn/), Shanghai, China.
- *2020.09 - 2023.06*, High School, [The Experimental High School Attached to Beijing Normal University](http://www.sdsz.com.cn/), Beijing, China.

<!-- 
# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)
- -->
# ğŸ¤ Academic Service
- Reviewer of ICLR 2026.

# ğŸ’» Internships
- *2025.7 - 2025.12*, AI Algorithm Engineer, [Huawei](https://www.huawei.com/cn/), Shanghai, China.